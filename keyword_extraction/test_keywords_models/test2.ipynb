{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e907f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "import time\n",
    "\n",
    "# different sections in a job description\n",
    "overall = \"\"\"\n",
    "A healthier future. It’s what drives us to innovate. To continuously advance science and ensure everyone has access to the healthcare they need today and for generations to come. Creating a world where we all have more time with the people we love. That’s what makes us Roche.\n",
    "\n",
    "Advances in AI, data, and computational sciences are transforming drug discovery and development. Roche’s Research and Early Development organisations at Genentech (gRED) and Pharma (pRED) have demonstrated how these technologies accelerate R&D, leveraging data and novel computational models to drive impact. Seamless data sharing and access to models across gRED and pRED are essential to maximising these opportunities. The new computational sciences Center of Excellence (CoE) is a strategic, unified group whose goal is to harness this transformative power of data and Artificial Intelligence (AI) to assist our scientists in both pRED and gRED to deliver more innovative and transformative medicines for patients worldwide.\n",
    "\n",
    "Within the CoE organisation, the Data and Digital Catalyst organisation drives the modernisation of our computational and data ecosystems and integration of digital technologies across Research and Early Development to enable our stakeholders, power data-driven science and accelerate decision-making.\n",
    "\n",
    "This internship position is located in South San Francisco, on-site.\n",
    "\n",
    "The Opportunity\n",
    "\n",
    "Our platform Identity & Access Management (IAM) team has developed the foundational tools needed to build scientific software solutions using security best practices in AWS, but often the scientific computing community at Roche does not utilize them fully because advanced capabilities appear complex. We’d love to make these advanced capabilities easy to use so the whole organization can leverage AWS, and Roche’s data, to its fullest, securely.\n",
    "\n",
    "We are looking for a junior engineer to help make these advanced concepts easy for the scientific community at Roche to understand and implement into their software solutions, through example solutions and architectures. This internship is to develop a suite of well-documented project templates, demonstrating solution architectures for advanced data access patterns in AWS, including Amazon Athena, Redshift, LakeFormation, and Bedrock.\n",
    "\"\"\"\n",
    "\n",
    "key_respon = \"\"\"\n",
    "Develop Solution Architectures: Create templates for individualized access to AWS Redshift (data warehouse) and AWS RDS (database).\n",
    "Implement Fine-Grained Access Control: Design patterns for AWS Athena using LakeFormation.\n",
    "Demonstrate Secure Agentic Patterns: Define state-of-the-art authentication & authorization for AI agents using AWS Bedrock and AgentCore.\n",
    "Documentation & Knowledge Sharing: Produce clear, scientific and engineering-centric documentation that allows other teams to easily adopt these advanced technologies and patterns to their work.\n",
    "\"\"\"\n",
    "\n",
    "required = \"\"\"\n",
    "You meet one of the following criteria:\n",
    "\n",
    "Must be pursuing or have attained an Associate's Degree.\n",
    "Must be pursuing a Bachelor's Degree (enrolled student).\n",
    "Must have attained a Bachelor's Degree (not currently enrolled in a graduate program).\n",
    "\n",
    "Required Majors: Computer Science, Information Systems, Data Engineering or a related field.\n",
    "\n",
    "Required Skills: \n",
    "\n",
    "Basic exposure or experience with developing infrastructure-as-code projects, ideally for user-facing web applications and/or data engineering pipelines.\n",
    "Must have experience with at least two programming languages, e.g.: JavaScript/TypeScript, Python, Rust, Go, SQL.\n",
    "Familiarity with standard software engineering tools such as git, npm, pip.\n",
    "Passion for best practices, architecture, software engineering and design.\n",
    "\"\"\"\n",
    "\n",
    "preferred = \"\"\"\n",
    "Excellent communication, collaboration, and interpersonal skills.\n",
    "Complements our culture and the standards that guide our daily behavior & decisions: Integrity, Courage, and Passion.\n",
    "Experience working with serverless and cloud-native architectures, ideally AWS, helps.\n",
    "Experience with biological data and processes, or working with scientists or in a research environment is a plus.\n",
    "Experience with Amazon Web Services (AWS) is a bonus, but not required.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96a58e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for KeyBERT with all-MiniLM-L6-v2: 0.25675647258758544\n",
      "\n",
      "Overall Keywords: [('scientific computing', 0.5074), ('data novel', 0.4953), ('scientific software', 0.4668), ('data computational', 0.4572), ('data ecosystems', 0.4437)]\n",
      "\n",
      "Key Responsibilities Keywords: [('aws rds', 0.5651), ('patterns aws', 0.5267), ('using aws', 0.5189), ('aws athena', 0.5167), ('access aws', 0.5117)]\n",
      "\n",
      "Required Skills Keywords: [('software engineering', 0.4889), ('required majors', 0.4303), ('required skills', 0.4156), ('computer science', 0.4054), ('experience programming', 0.4037)]\n",
      "\n",
      "Preferred Skills Keywords: [('services aws', 0.6037), ('aws', 0.5734), ('ideally aws', 0.5269), ('experience amazon', 0.5181), ('aws bonus', 0.5126)]\n"
     ]
    }
   ],
   "source": [
    "# loading default all-MiniLM-L6-v2 model (384 dimensional dense vector space)\n",
    "embedded_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "miniLM_model = KeyBERT(model=embedded_model)\n",
    "\n",
    "# run 10 times to get average time\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    keywords_overall = miniLM_model.extract_keywords(overall, keyphrase_ngram_range=(1, 2))\n",
    "    keywords_respon = miniLM_model.extract_keywords(key_respon, keyphrase_ngram_range=(1, 2))\n",
    "    keywords_required = miniLM_model.extract_keywords(required, keyphrase_ngram_range=(1, 2))\n",
    "    keywords_preferred = miniLM_model.extract_keywords(preferred, keyphrase_ngram_range=(1, 2))\n",
    "end = time.time()\n",
    "print(\"Average time for KeyBERT with all-MiniLM-L6-v2:\", (end - start) / 10)\n",
    "print()\n",
    "\n",
    "print(\"Overall Keywords:\", keywords_overall)\n",
    "print()\n",
    "print(\"Key Responsibilities Keywords:\", keywords_respon)\n",
    "print()\n",
    "print(\"Required Skills Keywords:\", keywords_required)\n",
    "print()\n",
    "print(\"Preferred Skills Keywords:\", keywords_preferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc113282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time for KeyBERT with all-mpnet-base-v2: 0.6158007144927978\n",
      "\n",
      "Overall Keywords: [('aws scientific', 0.4947), ('data ecosystems', 0.4772), ('scientific software', 0.4636), ('data novel', 0.4572), ('assist scientists', 0.4488)]\n",
      "\n",
      "Key Responsibilities Keywords: [('solution architectures', 0.5165), ('patterns aws', 0.483), ('warehouse aws', 0.4639), ('agentcore documentation', 0.4559), ('aws athena', 0.4385)]\n",
      "\n",
      "Required Skills Keywords: [('required skills', 0.4854), ('required majors', 0.4468), ('pipelines experience', 0.4314), ('software engineering', 0.4169), ('infrastructure code', 0.4078)]\n",
      "\n",
      "Preferred Skills Keywords: [('services aws', 0.4101), ('aws', 0.3936), ('skills', 0.3901), ('aws helps', 0.3847), ('architectures ideally', 0.3841)]\n"
     ]
    }
   ],
   "source": [
    "# loading all-mpnet-base-v2 model (768 dimensional dense vector space)\n",
    "embedded_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "mpnet_model = KeyBERT(model=embedded_model)\n",
    "\n",
    "\n",
    "# run 10 times to get average time\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    keywords_overall = mpnet_model.extract_keywords(overall, keyphrase_ngram_range=(1, 2))\n",
    "    keywords_respon = mpnet_model.extract_keywords(key_respon, keyphrase_ngram_range=(1, 2))\n",
    "    keywords_required = mpnet_model.extract_keywords(required, keyphrase_ngram_range=(1, 2))\n",
    "    keywords_preferred = mpnet_model.extract_keywords(preferred, keyphrase_ngram_range=(1, 2))\n",
    "end = time.time()\n",
    "print(\"Average time for KeyBERT with all-mpnet-base-v2:\", (end - start) / 10)\n",
    "print()\n",
    "\n",
    "print(\"Overall Keywords:\", keywords_overall)\n",
    "print()\n",
    "print(\"Key Responsibilities Keywords:\", keywords_respon)\n",
    "print()\n",
    "print(\"Required Skills Keywords:\", keywords_required)\n",
    "print()\n",
    "print(\"Preferred Skills Keywords:\", keywords_preferred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
